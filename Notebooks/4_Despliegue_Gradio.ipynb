{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (tabular_branch): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (combined_branch): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import dicomsdl\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Configuración del modelo preentrenado VGG16 y tamaño de imagen\n",
    "RESIZE_TO = (512, 512)\n",
    "\n",
    "# Modelo preentrenado VGG16 para heatmaps\n",
    "vgg16_model = VGG16(weights=\"imagenet\")\n",
    "grad_model = Model(inputs=vgg16_model.inputs, outputs=[vgg16_model.get_layer(\"block5_conv3\").output, vgg16_model.output])\n",
    "\n",
    "# Cargar el modelo local\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define tu clase de modelo combinado (ajusta si ya tienes esta definición)\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes=2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Rama de imágenes: ResNet18 preentrenado\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        num_resnet_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()  # Eliminar la capa final para usar embeddings\n",
    "\n",
    "        # Rama de características tabulares\n",
    "        self.tabular_branch = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Capa combinada\n",
    "        self.combined_branch = nn.Sequential(\n",
    "            nn.Linear(num_resnet_features + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, features):\n",
    "        # Procesar imágenes\n",
    "        image_features = self.resnet(image)\n",
    "        # Procesar características tabulares\n",
    "        tabular_features = self.tabular_branch(features)\n",
    "        # Combinar ambas ramas\n",
    "        combined_features = torch.cat((image_features, tabular_features), dim=1)\n",
    "        # Predicción final\n",
    "        output = self.combined_branch(combined_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Cargar modelo local\n",
    "model = CombinedModel(num_features=4)  # Número correcto de características\n",
    "model.load_state_dict(torch.load(\"cancer_classification_model_final.pth\", map_location=device, weights_only=True))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medias: age         58.897939\n",
      "biopsy       0.121492\n",
      "invasive     0.063003\n",
      "BIRADS       0.710136\n",
      "dtype: float64\n",
      "Desviaciones estándar: age         10.061221\n",
      "biopsy       0.326714\n",
      "invasive     0.242980\n",
      "BIRADS       0.606114\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "minority_class_size = train_df['cancer'].value_counts().min()\n",
    "\n",
    "# Definir la proporción de muestras de la clase mayoritaria (ejemplo: 2 veces la clase minoritaria)\n",
    "majority_class_multiplier = 10\n",
    "majority_class_size = minority_class_size * majority_class_multiplier\n",
    "\n",
    "# Obtener todas las muestras de la clase minoritaria\n",
    "minority_samples = train_df[train_df['cancer'] == 1]\n",
    "\n",
    "# Obtener una muestra aleatoria de la clase mayoritaria\n",
    "majority_samples = train_df[train_df['cancer'] == 0].sample(majority_class_size, random_state=42)\n",
    "\n",
    "# Combinar las muestras de ambas clases\n",
    "balanced_df = pd.concat([minority_samples, majority_samples]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "base_path='images_processed_heatmap'\n",
    "\n",
    "# saving image path into train dataframe\n",
    "balanced_df['img_path']= balanced_df.patient_id.astype(str)\\\n",
    "                    + '/' + balanced_df.image_id.astype(str)\\\n",
    "                    + '.png'\n",
    "\n",
    "\n",
    "base_path='images_processed_heatmap'\n",
    "\n",
    "# saving image path into train dataframe\n",
    "balanced_df['img_path']= balanced_df.patient_id.astype(str)\\\n",
    "                    + '/' + balanced_df.image_id.astype(str)\\\n",
    "                    + '.png'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "balanced_df['label'] = balanced_df['cancer']  # Usar 'cancer' como la etiqueta\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "train_df_mlo, val_df_mlo = train_test_split(balanced_df, test_size=0.2, random_state=42,stratify=balanced_df['label'] )\n",
    "\n",
    "training_tabular_features = train_df_mlo[['age', 'biopsy', 'invasive', 'BIRADS']]\n",
    "\n",
    "\n",
    "# Durante el entrenamiento\n",
    "mean = training_tabular_features.mean(axis=0)  # Media de cada columna\n",
    "std = training_tabular_features.std(axis=0)    # Desviación estándar de cada columna\n",
    "\n",
    "print(\"Medias:\", mean)\n",
    "print(\"Desviaciones estándar:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones de imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Configurar el normalizador\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(training_tabular_features)\n",
    "\n",
    "# Medias y desviaciones estándar calculadas durante el entrenamiento\n",
    "mean = np.array([58, 0.1, 0.0, 0.7])  # Ejemplo\n",
    "std = np.array([10, 0.3, 0.2, 0.6])  # Ejemplo\n",
    "\n",
    "# Función para preprocesar las variables tabulares\n",
    "def preprocess_user_input(age, biopsy, invasive, BIRADS):\n",
    "    user_input = np.array([age, biopsy, invasive, BIRADS])\n",
    "    # Normalizar usando las estadísticas del entrenamiento\n",
    "    normalized_input = (user_input - mean) / std\n",
    "    # Convertir a tensor\n",
    "    return torch.tensor(normalized_input, dtype=torch.float32).to(device)\n",
    "\n",
    "def process_image(image, age, density, invasive, BIRADS):\n",
    "    try:\n",
    "        # Determinar el formato de la imagen cargada\n",
    "        if hasattr(image, \"filename\") and image.filename.lower().endswith(\".dcm\"):\n",
    "            temp_dcm_path = \"temp_image.dcm\"\n",
    "            image.save(temp_dcm_path)\n",
    "            processed_ary = dicom_file_to_ary(temp_dcm_path)\n",
    "            image = Image.fromarray(processed_ary)\n",
    "\n",
    "        # Generar heatmap y superponerlo\n",
    "        heatmap = generate_heatmap(image)\n",
    "        transformed_image = apply_heatmap(image, heatmap)\n",
    "\n",
    "        # Convertir la imagen transformada a un tensor\n",
    "        transformed_image_tensor = transform(Image.fromarray(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB)))\n",
    "        transformed_image_tensor = transformed_image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        # Preprocesar las variables tabulares del usuario\n",
    "        tabular_data = preprocess_user_input(age, density, invasive, BIRADS).unsqueeze(0) \n",
    "\n",
    "        model.eval()\n",
    "        # Realizar la predicción\n",
    "        with torch.no_grad():\n",
    "            outputs = model(transformed_image_tensor, tabular_data)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0, class_idx].item()\n",
    "\n",
    "        # Clases de salida\n",
    "        classes = [\"No se evidencia Cáncer\", \"Se evidencia malignidad, remitir a estudios adicionales\"]\n",
    "        prediction = classes[class_idx]\n",
    "\n",
    "        # Convertir la imagen transformada con el heatmap para visualización\n",
    "        transformed_image_pil = Image.fromarray(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # return transformed_image_pil, f\"Predicción: {prediction} (Confianza: {confidence:.2f})\"\n",
    "        return transformed_image_pil, f\"Predicción: {prediction} (Confianza: {confidence:.2f})\"\n",
    "    except Exception as e:\n",
    "        return \"Error\", str(e)\n",
    "\n",
    "# Función para convertir DICOM a PNG\n",
    "def dicom_file_to_ary(path):\n",
    "    dcm_file = dicomsdl.open(str(path))\n",
    "    data = dcm_file.pixelData()\n",
    "\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "\n",
    "    # Redimensionar al tamaño especificado\n",
    "    image = Image.fromarray(data)\n",
    "    image = image.resize(RESIZE_TO, Image.LANCZOS)\n",
    "    return np.array(image, dtype=np.uint8)\n",
    "\n",
    "# Función para generar heatmap\n",
    "def generate_heatmap(image):\n",
    "    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    image_resized = cv2.resize(image_cv, (224, 224))\n",
    "    img_array = np.expand_dims(image_resized, axis=0)\n",
    "    img_array = tf.keras.applications.vgg16.preprocess_input(img_array)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    heatmap = tf.reduce_sum(weights * conv_outputs[0], axis=-1)\n",
    "\n",
    "    # Normalizar heatmap\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n",
    "    if isinstance(heatmap, tf.Tensor):  # Convertir a NumPy si es necesario\n",
    "        heatmap = heatmap.numpy()\n",
    "    heatmap = cv2.resize(heatmap, (image_cv.shape[1], image_cv.shape[0]))\n",
    "    heatmap = (heatmap * 255).astype(\"uint8\")\n",
    "    return heatmap\n",
    "\n",
    "# Función para superponer heatmap\n",
    "def apply_heatmap(image, heatmap, alpha=0.6):\n",
    "    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(heatmap_color, alpha, image_cv, 1 - alpha, 0)\n",
    "    return overlay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://1c4a791e97c4ca96a3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1c4a791e97c4ca96a3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n",
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n",
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n",
      "c:\\Users\\Melissa\\Ambientes\\venv_integrador_2\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Interfaz de Gradio\n",
    "interface = gr.Interface(\n",
    "    fn=process_image,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\"),  # Entrada como imagen PIL\n",
    "        gr.Number(label=\"Edad\"),  # Variable: Edad\n",
    "        gr.Number(label=\"Biopsia (1=Sí, 0=No)\"),  # Variable: Biopsia\n",
    "        gr.Number(label=\"Invasivo (1=Sí, 0=No)\"),  # Variable: Invasivo\n",
    "        gr.Number(label=\"BIRADS (0, 1 o 2 )\")  # Variable: BIRADS\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Imagen Transformada\"),  # Imagen con heatmap\n",
    "        gr.Text(label=\"Predicción\"),  # Predicción del modelo\n",
    "    ],\n",
    "    title=\"Detección de Cáncer con Imágenes Radiológicas\",\n",
    "    description=(\n",
    "        \"Esta herramienta fue creada con el propósito de contribuir a la detección temprana del cáncer de seno,\" \n",
    "        \" apoyando a profesionales médicos en su misión de salvar vidas. A través del análisis de imágenes radiológicas y variables clínicas,\" \n",
    "        \" este sistema combina la precisión de las redes neuronales con la pasión por el cuidado humano.\" \n",
    "        \" Subiendo una imagen y proporcionando información adicional, podrás obtener una predicción que puede marcar la diferencia en el diagnóstico y tratamiento de los pacientes.\" \n",
    "        \" Nuestro objetivo es ser un aliado en la lucha contra el cáncer, porque cada esfuerzo cuenta y cada vida importa.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_integrador_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
